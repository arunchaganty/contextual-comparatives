\section{Approach}

\subsection{Relevance classification}

Challenges -- huge search space.
Guidance -- principles of familiarity, semantic similarity and numeric proximity.
familiarity -- taken by assumption from the curated dataset.
semantic similarity -- learned, depends on not only the similarity to the mention, but also in composition of facts (e.g. "cost of an employee * people who died due to gun violence" never mix).
numeric proximity -- only consider within an order of magnitude. 

Generation by enumerating possibilities using the unit graph.

Given subjective nature, collect data for task using AMT.\@
In each, we present the crowd-worker with 4 expressions, rendering them with a generated perspective, and ask them to pick up to two of the options, or ``None of the above''. 
We considered only options that had a majority vote as positive, and the rest as negative.

Caveat, As posed, the provided options do influence the choice of relevant.

\paragraph{Learning.}
Train a system to rank expressions using the features learned.

\subsection{Perspective generation}

Generating natural language expression from logical forms can be quite difficult due to the extreme variability in generation.
Consider the following examples.
If our goal is to create output that aids the reader in comprehending the text, the generated perspective must itself be easy to comprehend.

We treat the problem as a translation problem between the logical form and the natural language utterance (the exact opposite of semantic parsing!).
We use Robin Jia's `seq2seq' package, implemented in Python.
TODO diagram of neural network.

To train the generation system, we had AMT translate about 8000 expressions to form 30000 expressions.

